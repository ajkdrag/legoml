{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee7abf8-0d2e-4454-afcc-6b204fa2e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "302340d9-df56-4c4b-ac1f-3a09ae9b5b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-17T04:44:10.973765Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mFinished logging setup        \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from legoml.utils.summary import summarize_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f308817-2785-4492-9dc8-41cbcf23d8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from legoml.utils.seed import set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4a9c829-f687-4c53-a08a-801b0d49a614",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7dbe24-b391-41a4-8def-2b097dd8db33",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f42e2d83-0bd7-4192-a0b2-bf3d8d9b8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68d33a4a-d52d-45f3-8b68-fe7bb006a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPURresidentLoader:\n",
    "    def __init__(self, images, labels, batch_size, shuffle=True, device=\"mps\"):\n",
    "        self.images = images.to(device)\n",
    "        self.labels = labels.to(device)\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.num_samples = images.shape[0]\n",
    "        self.num_batches = (self.num_samples + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            self.indices = torch.randperm(self.num_samples, device=self.images.device)\n",
    "        else:\n",
    "            self.indices = torch.arange(self.num_samples, device=self.images.device)\n",
    "        self.current_batch = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.current_batch >= self.num_batches:\n",
    "            raise StopIteration\n",
    "        \n",
    "        start_idx = self.current_batch * self.batch_size\n",
    "        end_idx = min(start_idx + self.batch_size, self.num_samples)\n",
    "        \n",
    "        batch_indices = self.indices[start_idx:end_idx]\n",
    "        batch_images = self.images[batch_indices]\n",
    "        batch_labels = self.labels[batch_indices]\n",
    "        \n",
    "        self.current_batch += 1\n",
    "        return batch_images, batch_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca88ac4-8e67-4bfe-9d41-f42788b3c585",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../../raw_data/\"\n",
    "batch_size = 128\n",
    "num_workers = 2\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root=data_path, train=True, download=True)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=data_path, train=False, download=True)\n",
    "\n",
    "train_images = torch.tensor(train_ds.data).permute(0, 3, 1, 2)\n",
    "train_labels = torch.tensor(train_ds.targets)\n",
    "test_images = torch.tensor(test_ds.data).permute(0, 3, 1, 2)\n",
    "test_labels = torch.tensor(test_ds.targets)\n",
    "\n",
    "train_loader = GPURresidentLoader(train_images, train_labels, batch_size, shuffle=True, device=device)\n",
    "test_loader = GPURresidentLoader(test_images, test_labels, batch_size, shuffle=False, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "858b4bf3-e571-4856-8f6f-33fc68d5441c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.images.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13775d8-cd7f-4145-9700-5a36bc0d752b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37b26640-795a-4962-936f-a5b2daca7519",
   "metadata": {},
   "source": [
    "## airbench stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "4d1b90e9-d847-4f01-bd1f-789885cff142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import ceil\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "#############################################\n",
    "#                DataLoader                 #\n",
    "#############################################\n",
    "\n",
    "CIFAR_MEAN = torch.tensor((0.4914, 0.4822, 0.4465))\n",
    "CIFAR_STD = torch.tensor((0.2470, 0.2435, 0.2616))\n",
    "\n",
    "def batch_flip_lr(inputs):\n",
    "    flip_mask = (torch.rand(len(inputs), device=inputs.device) < 0.5).view(-1, 1, 1, 1)\n",
    "    return torch.where(flip_mask, inputs.flip(-1), inputs)\n",
    "\n",
    "def batch_crop(images, crop_size):\n",
    "    r = (images.size(-1) - crop_size)//2\n",
    "    shifts = torch.randint(-r, r+1, size=(len(images), 2), device=images.device)\n",
    "    images_out = torch.empty((len(images), 3, crop_size, crop_size), device=images.device, dtype=images.dtype)\n",
    "    # The two cropping methods in this if-else produce equivalent results, but the second is faster for r > 2.\n",
    "    if r <= 2:\n",
    "        for sy in range(-r, r+1):\n",
    "            for sx in range(-r, r+1):\n",
    "                mask = (shifts[:, 0] == sy) & (shifts[:, 1] == sx)\n",
    "                images_out[mask] = images[mask, :, r+sy:r+sy+crop_size, r+sx:r+sx+crop_size]\n",
    "    else:\n",
    "        images_tmp = torch.empty((len(images), 3, crop_size, crop_size+2*r), device=images.device, dtype=images.dtype)\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 0] == s)\n",
    "            images_tmp[mask] = images[mask, :, r+s:r+s+crop_size, :]\n",
    "        for s in range(-r, r+1):\n",
    "            mask = (shifts[:, 1] == s)\n",
    "            images_out[mask] = images_tmp[mask, :, :, r+s:r+s+crop_size]\n",
    "    return images_out\n",
    "\n",
    "def make_random_square_masks(inputs, size):\n",
    "    is_even = int(size % 2 == 0)\n",
    "    n,c,h,w = inputs.shape\n",
    "\n",
    "    # seed top-left corners of squares to cutout boxes from, in one dimension each\n",
    "    corner_y = torch.randint(0, h-size+1, size=(n,), device=inputs.device)\n",
    "    corner_x = torch.randint(0, w-size+1, size=(n,), device=inputs.device)\n",
    "\n",
    "    # measure distance, using the center as a reference point\n",
    "    corner_y_dists = torch.arange(h, device=inputs.device).view(1, 1, h, 1) - corner_y.view(-1, 1, 1, 1)\n",
    "    corner_x_dists = torch.arange(w, device=inputs.device).view(1, 1, 1, w) - corner_x.view(-1, 1, 1, 1)\n",
    "    \n",
    "    mask_y = (corner_y_dists >= 0) * (corner_y_dists < size)\n",
    "    mask_x = (corner_x_dists >= 0) * (corner_x_dists < size)\n",
    "\n",
    "    final_mask = mask_y * mask_x\n",
    "\n",
    "    return final_mask\n",
    "\n",
    "def batch_cutout(inputs, size):\n",
    "    cutout_masks = make_random_square_masks(inputs, size)\n",
    "    return inputs.masked_fill(cutout_masks, 0)\n",
    "\n",
    "class CifarLoader:\n",
    "\n",
    "    def __init__(self, path, train=True, batch_size=500, aug=None, drop_last=None, shuffle=None, altflip=False):\n",
    "\n",
    "        data_path = os.path.join(path, 'train.pt' if train else 'test.pt')\n",
    "        if not os.path.exists(data_path):\n",
    "            dset = torchvision.datasets.CIFAR10(path, download=True, train=train)\n",
    "            images = torch.tensor(dset.data)\n",
    "            labels = torch.tensor(dset.targets)\n",
    "            torch.save({'images': images, 'labels': labels, 'classes': dset.classes}, data_path)\n",
    "        data = torch.load(data_path, map_location='mps')\n",
    "\n",
    "        self.epoch = 0\n",
    "        self.images, self.labels, self.classes = data['images'], data['labels'], data['classes']\n",
    "        # It's faster to load+process uint8 data than to load preprocessed fp16 data\n",
    "        self.images = (self.images.half() / 255).permute(0, 3, 1, 2).to(memory_format=torch.channels_last)\n",
    "\n",
    "        self.normalize = T.Normalize(CIFAR_MEAN, CIFAR_STD)\n",
    "        self.proc_images = {} # Saved results of image processing to be done on the first epoch\n",
    "\n",
    "        self.aug = aug or {}\n",
    "        for k in self.aug.keys():\n",
    "            assert k in ['flip', 'translate', 'cutout'], 'Unrecognized key: %s' % k\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = train if drop_last is None else drop_last\n",
    "        self.shuffle = train if shuffle is None else shuffle\n",
    "        self.altflip = altflip\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)//self.batch_size if self.drop_last else ceil(len(self.images)/self.batch_size)\n",
    "    \n",
    "    def __setattr__(self, k, v):\n",
    "        if k in ('images', 'labels'):\n",
    "            assert self.epoch == 0, 'Changing images or labels is only unsupported before iteration.'\n",
    "        super().__setattr__(k, v)\n",
    "\n",
    "    def __iter__(self):\n",
    "        print(\"iterating...\")\n",
    "        if self.epoch == 0:\n",
    "            images = self.proc_images['norm'] = self.normalize(self.images)\n",
    "            # Pre-flip images in order to do every-other epoch flipping scheme\n",
    "            if self.aug.get('flip', False):\n",
    "                images = self.proc_images['flip'] = batch_flip_lr(images)\n",
    "            # Pre-pad images to save time when doing random translation\n",
    "            pad = self.aug.get('translate', 0)\n",
    "            if pad > 0:\n",
    "                self.proc_images['pad'] = F.pad(images, (pad,)*4, 'reflect')\n",
    "\n",
    "        if self.aug.get('translate', 0) > 0:\n",
    "            images = batch_crop(self.proc_images['pad'], self.images.shape[-2])\n",
    "        elif self.aug.get('flip', False):\n",
    "            images = self.proc_images['flip']\n",
    "        else:\n",
    "            images = self.proc_images['norm']\n",
    "        # Flip all images together every other epoch. This increases diversity relative to random flipping\n",
    "        if self.aug.get('flip', False):\n",
    "            if self.altflip:\n",
    "                if self.epoch % 2 == 1:\n",
    "                    images = images.flip(-1)\n",
    "            else:\n",
    "                images = batch_flip_lr(images)\n",
    "        if self.aug.get('cutout', 0) > 0:\n",
    "            images = batch_cutout(images, self.aug['cutout'])\n",
    "\n",
    "        self.epoch += 1\n",
    "\n",
    "        indices = (torch.randperm if self.shuffle else torch.arange)(len(images), device=images.device)\n",
    "        for i in range(len(self)):\n",
    "            idxs = indices[i*self.batch_size:(i+1)*self.batch_size]\n",
    "            yield (images[idxs], self.labels[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d96ff-9440-4163-9a64-2e55f6a3f569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "fd0eb7d2-600e-48d9-8278-0570e1e4b5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = CifarLoader('../../raw_data/', train=True, aug=dict(flip=True, translate=4, cutout=16), batch_size=128)\n",
    "test_loader = CifarLoader('../../raw_data/', train=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c7eae-ba26-4b86-9059-936359648112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47eab8-653c-4265-9f30-02d713e65232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9910312c-fcb5-4612-b03d-879e90b5b82e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7505cca3-d64e-4c49-b082-c9ec373147cf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "87321e66-6005-4094-af21-79a5744dcb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.images[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "550e9143-d4d7-433d-822a-7ecf63bcaeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = model.half().to(\"mps\")\n",
    "net = net.to(memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "da018721-c1f5-49e5-9cb3-f15261bab02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim, sched = build_optim_and_sched(config, net, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e1a69bb9-9e47-44b0-a21d-c8eda29bd898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 122, 113)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(int(i * 255) for i in (0.49139968, 0.48215827, 0.44653124))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c9f70736-f393-4a9f-9ee2-8ffb5e81d37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from legoml.core.step_output import StepOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "7855a3a6-781e-40e0-9123-bd68c5e854ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(train_augmentation=False, max_epochs=20, data_root=\"../../raw_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3e3fd62f-2440-439d-9564-78b115c56785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(train_bs=64, eval_bs=32, train_augmentation=False, max_epochs=20, train_log_interval=100, eval_log_interval=100, data_root='../../raw_data/')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "91680766-6ff1-4bf7-b7b9-ab1ae4b35c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_optim_and_sched(\n",
    "    config: Config,\n",
    "    model: torch.nn.Module,\n",
    "    train_dl: DataLoader,\n",
    ") -> tuple[torch.optim.Optimizer, lrs.LRScheduler]:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=1e-6,\n",
    "        weight_decay=0.0005,\n",
    "    )\n",
    "    scheduler = lrs.OneCycleLR(\n",
    "        optimizer,\n",
    "        epochs=config.max_epochs,\n",
    "        steps_per_epoch=len(train_dl),\n",
    "        max_lr=1e-2,\n",
    "    )\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1bef2bfe-6349-412a-b224-036f835a7cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.image_clf.models import ConvMixer_w256_d8_p2_k7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "f0609a5e-47f5-40a3-8d2e-34e2429cb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMixer_w256_d8_p2_k7(c_in=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2237db37-c8d0-445c-9306-84b4e34f8d2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e0b3d3e9-e9fd-4584-b4e8-7de83457ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim, sched = build_optim_and_sched(config, model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1d92c1d9-2062-470e-8827-db51c11fba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.step_utils import forward_and_compute_loss, backward_and_step, log_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "179aacb2-07e1-4e51-b96e-a633f168c7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(\n",
    "    engine, batch, context\n",
    "):\n",
    "    config: Config = context.config\n",
    "    model = context.model\n",
    "    loss_fn = context.loss_fn\n",
    "    optimizer = context.optimizer\n",
    "    device = context.device\n",
    "    use_amp = context.scaler is not None\n",
    "\n",
    "    assert optimizer is not None, \"Optimizer is not set\"\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    inputs, targets = batch\n",
    "    outputs, loss = forward_and_compute_loss(\n",
    "        model, loss_fn, inputs.to(PRECISION), targets, device, use_amp\n",
    "    )\n",
    "\n",
    "    # scheduler step is handled explicity via engine event handles\n",
    "    backward_and_step(loss, optimizer, scaler=context.scaler)\n",
    "    log_step(engine, \"train\", config.train_log_interval)\n",
    "\n",
    "    return StepOutput(\n",
    "        loss=loss,\n",
    "        predictions=outputs.detach().cpu(),\n",
    "        targets=targets.detach().cpu(),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b0d80-7e29-452a-a41e-acf60a8cfb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "7472e8c9-a6e1-4fe5-8797-ca36f61c18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def eval_step(\n",
    "    engine, batch, context\n",
    "):\n",
    "    config: Config = context.config\n",
    "    model = context.model\n",
    "    loss_fn = context.loss_fn\n",
    "    device = context.device\n",
    "\n",
    "    model.eval()\n",
    "    inputs, targets = batch\n",
    "    outputs, loss = forward_and_compute_loss(model, loss_fn, inputs.to(PRECISION), targets, device)\n",
    "\n",
    "    log_step(engine, \"eval\", config.eval_log_interval)\n",
    "\n",
    "    return StepOutput(\n",
    "        loss=loss,\n",
    "        predictions=outputs.detach().cpu(),\n",
    "        targets=targets.detach().cpu(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6e869b18-6a46-4a89-a57f-6ddfb9559135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-09-17T07:34:36.797260Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mStarted experiment session    \u001b[0m \u001b[36mrun_dir\u001b[0m=\u001b[35mruns/train_img_clf_cifar10/run_20250917_130436\u001b[0m \u001b[36mrun_name\u001b[0m=\u001b[35mrun_20250917_130436\u001b[0m\n",
      "\u001b[2m2025-09-17T07:34:36.798562Z\u001b[0m [\u001b[33m\u001b[1mwarning  \u001b[0m] \u001b[1mForward pass failed for summary capture: Input type (c10::Half) and bias type (float) should be the same\u001b[0m\n",
      "-------------------------------------------------\n",
      "Name       | Type            | Params  | In | Out\n",
      "-------------------------------------------------\n",
      "stem       | Sequential      | 3.8 K   | ?  | ?  \n",
      "stem.0     | ConvActNorm     | 3.8 K   | ?  | ?  \n",
      "backbone   | Sequential      | 587.8 K | ?  | ?  \n",
      "backbone.0 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.1 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.2 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.3 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.4 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.5 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.6 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "backbone.7 | ConvMixerBlock  | 73.5 K  | ?  | ?  \n",
      "head       | Sequential      | 2.6 K   | ?  | ?  \n",
      "head.0     | GlobalAvgPool2d | 0       | ?  | ?  \n",
      "head.1     | Linear          | 2.6 K   | ?  | ?  \n",
      "-------------------------------------------------\n",
      "594.2 K        Trainable params\n",
      "0              Non-trainable params\n",
      "594.2 K        Total params\n",
      "2.267          Total estimated parameter size (MiB)\n",
      "\u001b[2m2025-09-17T07:34:36.801184Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mcheckpointing                 \u001b[0m \u001b[36mdir\u001b[0m=\u001b[35mruns/train_img_clf_cifar10/run_20250917_130436/artifacts/checkpoints\u001b[0m \u001b[36mevery_n_epochs\u001b[0m=\u001b[35m9999\u001b[0m\n",
      "iterating...\n",
      "\u001b[2m2025-09-17T07:34:44.265812Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoss: 1.6159591674804688      \u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mlr\u001b[0m=\u001b[35m[0.0003999999999999993]\u001b[0m \u001b[36mmode\u001b[0m=\u001b[35mtrain\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m100\u001b[0m\n",
      "\u001b[2m2025-09-17T07:34:51.495645Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mLoss: 1.5086994171142578      \u001b[0m \u001b[36mepoch\u001b[0m=\u001b[35m1\u001b[0m \u001b[36mlr\u001b[0m=\u001b[35m[0.0003999999999999993]\u001b[0m \u001b[36mmode\u001b[0m=\u001b[35mtrain\u001b[0m \u001b[36mstep\u001b[0m=\u001b[35m200\u001b[0m\n",
      "\u001b[2m2025-09-17T07:34:51.927530Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mEnded experiment session      \u001b[0m \u001b[36mrun_name\u001b[0m=\u001b[35mrun_20250917_130436\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[162], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m summarize_model(model, train_loader\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m], depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 42\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m sess\u001b[38;5;241m.\u001b[39mlog_params({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: asdict(config)})\n\u001b[1;32m     44\u001b[0m sess\u001b[38;5;241m.\u001b[39mlog_text(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(model))\n",
      "File \u001b[0;32m~/Workspace/private/legoml/src/legoml/core/engine.py:60\u001b[0m, in \u001b[0;36mEngine.loop\u001b[0;34m(self, dataloader, max_epochs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlocal_step \u001b[38;5;241m=\u001b[39m idx\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire(Events\u001b[38;5;241m.\u001b[39mSTEP_START, batch\u001b[38;5;241m=\u001b[39mbatch)\n\u001b[0;32m---> 60\u001b[0m op \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m op\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfire(Events\u001b[38;5;241m.\u001b[39mSTEP_END, batch\u001b[38;5;241m=\u001b[39mbatch)\n",
      "Cell \u001b[0;32mIn[160], line 22\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(engine, batch, context)\u001b[0m\n\u001b[1;32m     17\u001b[0m outputs, loss \u001b[38;5;241m=\u001b[39m forward_and_compute_loss(\n\u001b[1;32m     18\u001b[0m     model, loss_fn, inputs\u001b[38;5;241m.\u001b[39mto(PRECISION), targets, device, use_amp\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# scheduler step is handled explicity via engine event handles\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mbackward_and_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m log_step(engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, config\u001b[38;5;241m.\u001b[39mtrain_log_interval)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m StepOutput(\n\u001b[1;32m     26\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss,\n\u001b[1;32m     27\u001b[0m     predictions\u001b[38;5;241m=\u001b[39moutputs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     28\u001b[0m     targets\u001b[38;5;241m=\u001b[39mtargets\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),\n\u001b[1;32m     29\u001b[0m )\n",
      "File \u001b[0;32m~/Workspace/private/legoml/experiments/step_utils.py:24\u001b[0m, in \u001b[0;36mbackward_and_step\u001b[0;34m(loss, optimizer, scheduler, scaler)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/private/legoml/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:465\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    462\u001b[0m     \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    463\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 465\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Workspace/private/legoml/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:359\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    353\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    358\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    360\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/Workspace/private/legoml/.venv/lib/python3.10/site-packages/torch/amp/grad_scaler.py:359\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_maybe_opt_step\u001b[39m(\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    353\u001b[0m     optimizer: torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mOptimizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    358\u001b[0m     retval: Optional[\u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    360\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with run(base_dir=Path(\"runs\").joinpath(\"train_img_clf_cifar10\")) as sess:\n",
    "    train_context = Context(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "        optimizer=optim,\n",
    "        scheduler=sched,\n",
    "        device=device,\n",
    "        scaler=torch.GradScaler(device=device.type),  # slow on M1 air\n",
    "    )\n",
    "    trainer = Engine(train_step, train_context)\n",
    "\n",
    "    eval_context = Context(\n",
    "        config=config,\n",
    "        model=model,\n",
    "        loss_fn=torch.nn.CrossEntropyLoss(),\n",
    "        device=device,\n",
    "    )\n",
    "    evaluator = Engine(\n",
    "        eval_step,\n",
    "        eval_context,\n",
    "        callbacks=[\n",
    "            MetricsCallback(metrics=[MultiClassAccuracy(\"eval_acc\")]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    trainer.callbacks.extend(\n",
    "        [\n",
    "            EvalOnEpochEndCallback(evaluator, test_loader, 1),\n",
    "            MetricsCallback(metrics=[MultiClassAccuracy(\"train_acc\")]),\n",
    "            CheckpointCallback(\n",
    "                dirpath=sess.get_artifact_dir().joinpath(\"checkpoints\"),\n",
    "                save_every_n_epochs=9999,\n",
    "                save_on_engine_end=True,\n",
    "                best_fn=lambda: evaluator.state.metrics[\"eval_acc\"],\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    summarize_model(model, train_loader.images[0], depth=2)\n",
    "    model.to(device)\n",
    "    trainer.loop(train_loader, max_epochs=config.max_epochs)\n",
    "    sess.log_params({\"exp_config\": asdict(config)})\n",
    "    sess.log_text(\"model\", str(model))\n",
    "    sess.log_params({\"trainer\": trainer.to_dict()})\n",
    "    sess.log_params({\"evaluator\": evaluator.to_dict()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697291c5-4d54-4259-b7de-870d7fdd78dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759fb8b2-a329-4250-b71f-5e59b433fe87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811187d4-e9da-499e-8f34-daffa7b8dde1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9aa815-3284-4cd5-990d-30ff1c7b6e61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
